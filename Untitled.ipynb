{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "from biomedicus.sentences.vocabulary import Vocabulary\n",
    "vocabulary = Vocabulary('/Users/benknoll/BIOMEDICUS_DATA/sentences/vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs: ['The quick brown fox jumps\\n over the lazy dog.', 'Simplify, then add lightness.']\n",
      "(2,)\n",
      "tokens: <tf.RaggedTensor [[b'The', b'quick', b'brown', b'fox', b'jumps', b'over', b'the', b'lazy', b'dog.'], [b'Simplify,', b'then', b'add', b'lightness.']]>\n",
      "starts: <tf.RaggedTensor [[0, 4, 10, 16, 20, 27, 32, 36, 41], [0, 10, 15, 19]]>\n",
      "ends: <tf.RaggedTensor [[3, 9, 15, 19, 25, 31, 35, 40, 45], [9, 14, 18, 29]]>\n",
      "priors: <tf.RaggedTensor [[b'', b' ', b' ', b' ', b' ', b'\\n ', b' ', b' ', b' '], [b'', b' ', b' ', b' ']]>\n",
      "posts: <tf.RaggedTensor [[b' ', b' ', b' ', b' ', b'\\n ', b' ', b' ', b' ', b''], [b' ', b' ', b' ', b'']]>\n",
      "prev_token_marker: <tf.RaggedTensor [[[3], [3], [3], [3], [3], [3], [3], [3], [3]], [[3], [3], [3], [3]]]>\n",
      "token_char_ids <tf.RaggedTensor [[[63, 83, 80], [92, 96, 84, 78, 86], [77, 93, 90, 98, 89], [81, 90, 99], [85, 96, 88, 91, 94], [90, 97, 80, 93], [95, 83, 80], [87, 76, 101, 100], [79, 90, 82, 25]], [[62, 84, 88, 91, 87, 84, 81, 100, 23], [95, 83, 80, 89], [76, 79, 79], [87, 84, 82, 83, 95, 89, 80, 94, 94, 25]]]>\n",
      "token_start_marker <tf.RaggedTensor [[[1], [1], [1], [1], [1], [1], [1], [1], [1]], [[1], [1], [1], [1]]]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[3, 1, 63, 83, 80, 2, 9, 1], [3, 9, 1, 92, 96, 84, 78, 86, 2, 9, 1], [3, 9, 1, 77, 93, 90, 98, 89, 2, 9, 1], [3, 9, 1, 81, 90, 99, 2, 9, 1], [3, 9, 1, 85, 96, 88, 91, 94, 2, 7, 9, 1], [3, 7, 9, 1, 90, 97, 80, 93, 2, 9, 1], [3, 9, 1, 95, 83, 80, 2, 9, 1], [3, 9, 1, 87, 76, 101, 100, 2, 9, 1], [3, 9, 1, 79, 90, 82, 25, 2, 1]], [[3, 1, 62, 84, 88, 91, 87, 84, 81, 100, 23, 2, 9, 1], [3, 9, 1, 95, 83, 80, 89, 2, 9, 1], [3, 9, 1, 76, 79, 79, 2, 9, 1], [3, 9, 1, 87, 84, 82, 83, 95, 89, 80, 94, 94, 25, 2, 1]]]>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys, values = zip(*vocabulary._character_to_id.items())\n",
    "char_table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys, values), 11)\n",
    "\n",
    "docs = [\"The quick brown fox jumps\\n over the lazy dog.\", \"Simplify, then add lightness.\"]\n",
    "print('docs:', docs)\n",
    "print(tf.constant(docs).shape)\n",
    "tokenizer = tensorflow_text.WhitespaceTokenizer()\n",
    "tokens, starts, ends = tokenizer.tokenize_with_offsets(docs)\n",
    "print('tokens:', tokens)\n",
    "print('starts:', starts)\n",
    "print('ends:', ends)\n",
    "first_prev_end = tf.fill([tokens.nrows(), 1], tf.cast(0, tf.int64))\n",
    "prev_ends = tf.concat([first_prev_end, ends[:, :-1]], -1)\n",
    "prior_lens = starts - prev_ends\n",
    "priors = tf.strings.substr(tf.expand_dims(docs, -1), prev_ends.to_tensor(), prior_lens.to_tensor())\n",
    "priors = tf.RaggedTensor.from_tensor(priors, lengths=prev_ends.row_lengths())\n",
    "print('priors:', priors)\n",
    "last_next_start = tf.fill(first_prev_end.shape, tf.int64.max)\n",
    "next_starts = tf.concat([starts[:, 1:], last_next_start], -1)\n",
    "post_lens = next_starts - ends\n",
    "posts = tf.strings.substr(tf.expand_dims(docs, -1), ends.to_tensor(), post_lens.to_tensor())\n",
    "posts = tf.RaggedTensor.from_tensor(posts, lengths=post_lens.row_lengths())\n",
    "print('posts:', posts)\n",
    "prev_token_marker = tf.expand_dims(tf.RaggedTensor.from_tensor(tf.fill(tokens.bounding_shape(), tf.cast(3, tf.int32)), lengths=tokens.row_lengths()), -1)\n",
    "print('prev_token_marker:', prev_token_marker)\n",
    "token_start_marker = tf.RaggedTensor.from_row_lengths(tf.fill(prev_token_marker.flat_values.shape, tf.cast(1, tf.int32)), prev_token_marker.row_lengths())\n",
    "token_end_marker = tf.RaggedTensor.from_row_lengths(tf.fill(prev_token_marker.flat_values.shape, tf.cast(2, tf.int32)), prev_token_marker.row_lengths())\n",
    "next_token_marker = tf.RaggedTensor.from_row_lengths(tf.fill(prev_token_marker.flat_values.shape, tf.cast(1, tf.int32)), prev_token_marker.row_lengths())\n",
    "prior_char_ids = tf.ragged.map_flat_values(char_table.lookup, tf.strings.unicode_split(priors, 'UTF-8'))\n",
    "token_char_ids = tf.ragged.map_flat_values(char_table.lookup, tf.strings.unicode_split(tokens, 'UTF-8'))\n",
    "post_char_ids = tf.ragged.map_flat_values(char_table.lookup, tf.strings.unicode_split(posts, 'UTF-8'))\n",
    "print('token_char_ids', token_char_ids)\n",
    "print('token_start_marker', token_start_marker)\n",
    "tf.concat([\n",
    "    prev_token_marker,\n",
    "    prior_char_ids,\n",
    "    token_start_marker,\n",
    "    token_char_ids,\n",
    "    token_end_marker,\n",
    "    post_char_ids,\n",
    "    next_token_marker\n",
    "], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
